{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3c49da",
   "metadata": {},
   "source": [
    "# 강의자료의 문제: 서울시 열린데이터광장의 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv```를 읽어서 데이터프레임으로 만들고 다음을 답하시오.\n",
    "\n",
    "\n",
    "\n",
    "(1) 전/후반기의 반기별 대여건수 합계, 평균을 출력\n",
    "\n",
    "(2) 년월별 대여건수를 계산하고 선그래프 출력\n",
    "\n",
    "    - y축 대여건수\n",
    "\n",
    "    - x축 년월별 (2018년 1월, 2월, ... , 12월, 2019년 1월, ... 이런 순서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed12e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/20 00:41:53 WARN Utils: Your hostname, sojaehwiui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.29 instead (on interface en0)\n",
      "23/11/20 00:41:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/20 00:41:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade9c6ca",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6164d13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_bicycle = spark.read.format('csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21099e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abbc2381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+\n",
      "|      Date|Count|year|month|\n",
      "+----------+-----+----+-----+\n",
      "|2018-01-01| 4950|2018|   01|\n",
      "|2018-01-02| 7136|2018|   01|\n",
      "|2018-01-03| 7156|2018|   01|\n",
      "|2018-01-04| 7102|2018|   01|\n",
      "|2018-01-05| 7705|2018|   01|\n",
      "+----------+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"year\", bicycle.Date.substr(1, 4))\n",
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))\n",
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08c71945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상반기/하반기 구분 UDF 함수 만들기\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def classifyQuarter(s):\n",
    "    s=int(s)\n",
    "    if 1<=s and s< 7:\n",
    "        return \"1H\"\n",
    "    else:\n",
    "        return \"2H\"\n",
    "\n",
    "\n",
    "quarter_udf = udf(classifyQuarter, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98daa5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|quarter|count|\n",
      "+-------+-----+\n",
      "|     1H|  271|\n",
      "|     2H|  184|\n",
      "+-------+-----+\n",
      "\n",
      "+----------+-----+----+-----+-------+\n",
      "|      Date|Count|year|month|quarter|\n",
      "+----------+-----+----+-----+-------+\n",
      "|2018-01-01| 4950|2018|   01|     1H|\n",
      "|2018-01-02| 7136|2018|   01|     1H|\n",
      "|2018-01-03| 7156|2018|   01|     1H|\n",
      "|2018-01-04| 7102|2018|   01|     1H|\n",
      "|2018-01-05| 7705|2018|   01|     1H|\n",
      "+----------+-----+----+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 날짜를 반기단위로 분류하여, 일 수 파악\n",
    "bicycle = bicycle.withColumn(\"quarter\", quarter_udf(bicycle.month))\n",
    "bicycle.groupBy('quarter').count().show()\n",
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffa725",
   "metadata": {},
   "source": [
    "# (1) 전/후반기의 반기별 대여건수 합계, 평균을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c353be25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|quarter|sum(count)|\n",
      "+-------+----------+\n",
      "|     1H|   5528321|\n",
      "|     2H|   6468488|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상반기/하반기 별 Count를 집계\n",
    "bicycle.groupBy('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89bccf3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|quarter|       avg(count)|\n",
      "+-------+-----------------+\n",
      "|     1H|20399.70848708487|\n",
      "|     2H|35154.82608695652|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 상반기/하반기 별 평균을 집계\n",
    "bicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4abd4a4",
   "metadata": {},
   "source": [
    "# (2) 년월별 대여건수를 계산하고 선그래프 출력\n",
    "\n",
    "- y축 대여건수\n",
    "\n",
    "- x축 년월별 (2018년 1월, 2월, ... , 12월, 2019년 1월, ... 이런 순서)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "487918ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+\n",
      "|year|month|sum(count)|\n",
      "+----+-----+----------+\n",
      "|2018|   01|    164367|\n",
      "|2018|   02|    168741|\n",
      "|2018|   03|    462661|\n",
      "|2018|   04|    687885|\n",
      "|2018|   05|    965609|\n",
      "|2018|   06|   1207123|\n",
      "|2018|   07|   1100015|\n",
      "|2018|   08|   1037505|\n",
      "|2018|   09|   1447993|\n",
      "|2018|   10|   1420621|\n",
      "|2018|   11|    961532|\n",
      "|2018|   12|    500822|\n",
      "|2019|   01|    495573|\n",
      "|2019|   02|    471543|\n",
      "|2019|   03|    904819|\n",
      "+----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "_statistics = bicycle.groupBy('year', 'month').agg({\"count\":\"sum\"}).orderBy('year', 'month')\n",
    "_statistics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f18c92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sum(count): long (nullable = true)\n",
      " |-- year-month: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|sum(count)|year-month|\n",
      "+----------+----------+\n",
      "|    164367|   2018-01|\n",
      "|    168741|   2018-02|\n",
      "|    462661|   2018-03|\n",
      "|    687885|   2018-04|\n",
      "|    965609|   2018-05|\n",
      "|   1207123|   2018-06|\n",
      "|   1100015|   2018-07|\n",
      "|   1037505|   2018-08|\n",
      "|   1447993|   2018-09|\n",
      "|   1420621|   2018-10|\n",
      "|    961532|   2018-11|\n",
      "|    500822|   2018-12|\n",
      "|    495573|   2019-01|\n",
      "|    471543|   2019-02|\n",
      "|    904819|   2019-03|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def yearMonthConcat(year, month):\n",
    "    return year + \"-\" + month\n",
    "    \n",
    "yearMonthConcatUdf = udf(yearMonthConcat, StringType())\n",
    "_statistics = _statistics.withColumn(\"year-month\", yearMonthConcatUdf(_statistics.year, _statistics.month))   \n",
    "_statistics = _statistics.drop(\"year\")\n",
    "_statistics = _statistics.drop(\"month\")\n",
    "_statistics.printSchema()\n",
    "_statistics.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94abb75c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_statistics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/v4/z0sc2ps925v8zh04gvwkx_500000gn/T/ipykernel_99454/1526285404.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpandas_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_statistics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"year-month\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpandas_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sum(count)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Year-Month'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_statistics' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pandas_df=_statistics.toPandas()\n",
    "plt.plot(pandas_df[\"year-month\"], pandas_df[\"sum(count)\"])\n",
    "plt.xlabel('Year-Month')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d74df2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
