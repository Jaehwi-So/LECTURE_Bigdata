{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "215f7581",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 기계학습\n",
    "\n",
    "ML에 사용할 수 있도록 변환한 데이터들을 다양한 모델을 활용하여 지도학습을 통해 예측을 수행해보자\n",
    "해당 Jupyter Notebook에서는 다음 내용들을 다룬다.\n",
    "\n",
    "1. 서포트 벡터머신(Support Vector Machine)\n",
    "2. 로지스틱 회귀(Logistic Regression)\n",
    "3. 네이브 베이시안(Naive Bayes)\n",
    "4. 의사결정 트리(Decision Tree)\n",
    "<br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cd2e25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 04:43:55 WARN Utils: Your hostname, sojaehwiui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 172.30.1.29 instead (on interface en0)\n",
      "23/12/19 04:43:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/19 04:43:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3d8377d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 04:43:56 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fsvm=os.path.join(os.getcwd(),'data','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9491140d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0afb6a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1) #Label과 Features로 구성되어 있다. Label + Features(Dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193908ac",
   "metadata": {},
   "source": [
    "### Label to String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09f868bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------+\n",
      "|label|            features|labelStr|\n",
      "+-----+--------------------+--------+\n",
      "|  0.0|(692,[127,128,129...|       0|\n",
      "|  1.0|(692,[158,159,160...|       1|\n",
      "|  1.0|(692,[124,125,126...|       1|\n",
      "|  1.0|(692,[152,153,154...|       1|\n",
      "|  1.0|(692,[151,152,153...|       1|\n",
      "+-----+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, IntegerType\n",
    "_df=dfsvm.withColumn('labelStr', dfsvm.label.cast(IntegerType()).cast(StringType()))\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11097cdc",
   "metadata": {},
   "source": [
    "### MLUtils를 이용하여 RDD 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14388bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "data = MLUtils.loadLibSVMFile(spark.sparkContext, fsvm)\n",
    "label = data.map(lambda x: x.label)\n",
    "features = data.map(lambda x: x.features)\n",
    "\n",
    "label.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5487e1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f7b19",
   "metadata": {},
   "source": [
    "### Train, Test Set 분할하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e3d985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   43|\n",
      "|  1.0|   57|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train, test = dfsvm.randomSplit([0.6,0.4])\n",
    "dfsvm.groupby('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3de287",
   "metadata": {},
   "source": [
    "## 모델 적용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64cc95",
   "metadata": {},
   "source": [
    "### 1. Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84da78c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1) #모델 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a4252278",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 04:44:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/12/19 04:44:04 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.VectorBLAS\n"
     ]
    }
   ],
   "source": [
    "lsvcModel = lsvc.fit(train) #훈련데이터에 대해서 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89e2d3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDf = lsvcModel.transform(test) #테스트셋에 대해서 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f1e0461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.select('label', 'prediction').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4a740e",
   "metadata": {},
   "source": [
    "### 2. 로지스틱 회귀\n",
    "이진 분류를 수행해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adcca885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(train)\n",
    "testDf = lrModel.transform(test)\n",
    "testDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e25ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+----------+\n",
      "|label|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|  0.0|[0.97401772358881...|[0.72591959058934...|       0.0|\n",
      "|  0.0|[0.60872904769431...|[0.64765082619373...|       0.0|\n",
      "|  0.0|[0.99306355311509...|[0.72969260659674...|       0.0|\n",
      "|  0.0|[1.00268802621131...|[0.73158674830029...|       0.0|\n",
      "|  0.0|[0.91463076375201...|[0.71394682481350...|       0.0|\n",
      "|  0.0|[0.99978333499413...|[0.73101597757179...|       0.0|\n",
      "|  0.0|[0.81270585248836...|[0.69268580514083...|       0.0|\n",
      "|  0.0|[0.99119625838965...|[0.72932414101630...|       0.0|\n",
      "|  0.0|[0.94748028228275...|[0.72060816021990...|       0.0|\n",
      "|  0.0|[0.95876875042810...|[0.72287522073720...|       0.0|\n",
      "|  0.0|[0.82460695680365...|[0.69521339313490...|       0.0|\n",
      "|  0.0|[0.95562130296550...|[0.72224426108532...|       0.0|\n",
      "|  0.0|[0.96053938098069...|[0.72322978518508...|       0.0|\n",
      "|  0.0|[0.87098315358903...|[0.70495023062137...|       0.0|\n",
      "|  0.0|[0.77443835100467...|[0.68448021521658...|       0.0|\n",
      "|  0.0|[0.53386977898391...|[0.63038522333004...|       0.0|\n",
      "|  0.0|[0.16498869057610...|[0.54115385994965...|       0.0|\n",
      "|  1.0|[-1.1075440807967...|[0.24832903127093...|       1.0|\n",
      "|  1.0|[0.19258357005735...|[0.54799763770769...|       0.0|\n",
      "|  1.0|[-1.3603911351253...|[0.20417673993183...|       1.0|\n",
      "|  1.0|[-1.1373610788074...|[0.24280519951057...|       1.0|\n",
      "|  1.0|[-1.4255535531960...|[0.19379243771284...|       1.0|\n",
      "|  1.0|[-0.6975110683043...|[0.33236428684076...|       1.0|\n",
      "|  1.0|[-1.0688016208052...|[0.25563104955259...|       1.0|\n",
      "|  1.0|[-1.3304580361043...|[0.20908361076384...|       1.0|\n",
      "|  1.0|[-1.4777620244050...|[0.18576569028698...|       1.0|\n",
      "|  1.0|[-0.6527883422544...|[0.34236146614770...|       1.0|\n",
      "|  1.0|[-1.2591878919041...|[0.22111372378445...|       1.0|\n",
      "|  1.0|[-1.3297142200299...|[0.20920664049995...|       1.0|\n",
      "|  1.0|[-1.1442313835498...|[0.24154432118967...|       1.0|\n",
      "|  1.0|[-1.1515814813750...|[0.24020033758368...|       1.0|\n",
      "|  1.0|[-1.2171965183349...|[0.22843018753402...|       1.0|\n",
      "|  1.0|[-1.0647016435969...|[0.25641199019759...|       1.0|\n",
      "|  1.0|[-1.0212191034092...|[0.26479000200155...|       1.0|\n",
      "|  1.0|[-0.9644219447419...|[0.27599372195909...|       1.0|\n",
      "|  1.0|[-1.2522155017153...|[0.22231686049978...|       1.0|\n",
      "|  1.0|[-1.2909405855289...|[0.21569364951321...|       1.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDf.select('label','rawPrediction','probability','prediction').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b4b4b",
   "metadata": {},
   "source": [
    "### 3. Naive Bayes\n",
    "조건부 베이지안 확률에 따라 분류하는 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86bba23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "|  1.0|       1.0|\n",
      "+-----+----------+\n",
      "\n",
      "Test set accuracy = 1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "\n",
    "nb=NaiveBayes(featuresCol='features', labelCol='label', modelType='multinomial', predictionCol='prediction')\n",
    "#nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "model = nb.fit(train)\n",
    "\n",
    "predictions=model.transform(test)\n",
    "\n",
    "predictions.select('label', 'prediction').show(100)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "# compute accuracy on the test set\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test set accuracy = \" + str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1cd52b",
   "metadata": {},
   "source": [
    "### 4. Decision Tree\n",
    "조건에 따라 분기하는 의사결정 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7fcbf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/19 04:44:06 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dfsvm = spark.read.format(\"libsvm\").load(\"data/sample_libsvm_data.txt\")\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(dfsvm)\n",
    "\n",
    "# maxCategories > 4보다 크면 연속값\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "534a8168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       1.0|         1.0|(692,[100,101,102...|\n",
      "|       1.0|         1.0|(692,[121,122,123...|\n",
      "|       1.0|         1.0|(692,[122,123,124...|\n",
      "|       1.0|         1.0|(692,[123,124,125...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[124,125,126...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[126,127,128...|\n",
      "|       1.0|         1.0|(692,[127,128,129...|\n",
      "|       1.0|         1.0|(692,[128,129,130...|\n",
      "|       1.0|         1.0|(692,[129,130,131...|\n",
      "|       1.0|         1.0|(692,[152,153,154...|\n",
      "|       0.0|         0.0|(692,[97,98,99,12...|\n",
      "|       0.0|         0.0|(692,[119,120,121...|\n",
      "|       0.0|         0.0|(692,[123,124,125...|\n",
      "|       0.0|         0.0|(692,[124,125,126...|\n",
      "|       0.0|         0.0|(692,[125,126,127...|\n",
      "|       0.0|         0.0|(692,[125,126,153...|\n",
      "|       0.0|         0.0|(692,[126,127,128...|\n",
      "|       0.0|         0.0|(692,[127,128,129...|\n",
      "|       0.0|         0.0|(692,[127,128,129...|\n",
      "|       0.0|         0.0|(692,[128,129,130...|\n",
      "|       0.0|         0.0|(692,[129,130,131...|\n",
      "|       0.0|         0.0|(692,[129,130,131...|\n",
      "|       0.0|         0.0|(692,[129,130,131...|\n",
      "|       0.0|         0.0|(692,[156,157,158...|\n",
      "|       0.0|         0.0|(692,[158,159,160...|\n",
      "|       0.0|         0.0|(692,[158,159,160...|\n",
      "+----------+------------+--------------------+\n",
      "\n",
      "Test Error = 0 \n",
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_951a69c954b3, depth=2, numNodes=5, numClasses=2, numFeatures=692\n"
     ]
    }
   ],
   "source": [
    "(train, test) = dfsvm.randomSplit([0.7, 0.3])\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
    "\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
    "\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(100)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
    "\n",
    "treeModel = model.stages[2]\n",
    "\n",
    "print(treeModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e8169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
